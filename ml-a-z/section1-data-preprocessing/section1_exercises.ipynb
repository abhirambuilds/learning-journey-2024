{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGXsZ7kfqMqV"
      },
      "outputs": [],
      "source": [
        "###Coding Exercise 1: Importing and Preprocessing a Dataset for Machine Learning\n",
        "\n",
        "\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "# Loading the Iris dataset\n",
        "dataset = pd.read_csv('iris.csv')\n",
        "\n",
        "# Creating the matrix of features (X) and the dependent variable vector (y)\n",
        "X = dataset.iloc[: , :-1].values\n",
        "y = dataset.iloc[: , -1].values\n",
        "\n",
        "# Printing the matrix of features and the dependent variable vector\n",
        "\n",
        "print(X)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###Coding Exercise 2: Handling Missing Data in a Dataset for Machine Learning\n",
        "\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load the dataset\n",
        "dataset = pd.read_csv('pima-indians-diabetes.csv')\n",
        "# Identify missing data (assumes that missing data is represented as NaN)\n",
        "missing_data = dataset.isnull().sum()\n",
        "# Print the number of missing entries in each column\n",
        "print(missing_data)\n",
        "# Configure an instance of the SimpleImputer class\n",
        "imputer = SimpleImputer(missing_values=np.nan , strategy=\"mean\" )\n",
        "# Fit the imputer on the DataFrame\n",
        "imputer.fit(dataset)\n",
        "# Apply the transform to the DataFrame\n",
        "transform = imputer.transform(dataset)\n",
        "#Print your updated matrix of features\n",
        "print(transform)"
      ],
      "metadata": {
        "id": "4HkhjjPwrLuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Coding Exercise 3: Encoding Categorical Data for Machine Learning\n",
        "\n",
        "\n",
        "# Importing the necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder , LabelEncoder\n",
        "# Load the dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Identify the categorical data\n",
        "categorical_features = ['Sex', 'Embarked', 'Pclass']\n",
        "\n",
        "# Implement an instance of the ColumnTransformer class\n",
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_features)], remainder='passthrough')\n",
        "\n",
        "# Apply the fit_transform method on the instance of ColumnTransformer\n",
        "X= ct.fit_transform(df)\n",
        "\n",
        "# Convert the output into a NumPy array\n",
        "X = np.array(X)\n",
        "\n",
        "# Use LabelEncoder to encode binary categorical data\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df['Survived'])\n",
        "# Print the updated matrix of features and the dependent variable vector\n",
        "print(\"Updated matrix of features: \\n\", X)\n",
        "print(\"Updated dependent variable vector: \\n\", y)\n"
      ],
      "metadata": {
        "id": "SvVaR3UWrkpC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Coding Exercise 4: Dataset Splitting and Feature Scaling\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# Load the Iris dataset\n",
        "df = pd.read_csv('iris.csv')\n",
        "# Separate features and target\n",
        "X = df.iloc[:,:-1].values\n",
        "Y = df.iloc[:,-1].values\n",
        "# Split the dataset into an 80-20 training-test set\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , Y , test_size=0.2 , random_state=42)\n",
        "# Apply feature scaling on the training and test sets\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Print the scaled training and test sets\n",
        "print(\"Scaled Training Set:\")\n",
        "print(X_train)\n",
        "print(\"\\nScaled Test Set:\")\n",
        "print(X_test)\n"
      ],
      "metadata": {
        "id": "bqlrzNDmsB1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Coding exercise 5: Feature scaling for Machine Learning\n",
        "\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Load the Wine Quality Red dataset\n",
        "df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
        "\n",
        "# Separate features and target\n",
        "X = df.iloc[: , :-1].values\n",
        "y = df.iloc[: , -1].values\n",
        "\n",
        "# Split the dataset into an 80-20 training-test set\n",
        "X_train , X_test , y_train , y_test = train_test_split(X , y , test_size= 0.2 , random_state = 42 )\n",
        "\n",
        "# Create an instance of the StandardScaler class\n",
        "sc = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler on the features from the training set and transform it\n",
        "X_train=sc.fit_transform(X_train)\n",
        "\n",
        "# Apply the transform to the test set\n",
        "X_test=sc.transform(X_test)\n",
        "\n",
        "# Print the scaled training and test datasets\n",
        "print(X_train)\n",
        "print(X_test)"
      ],
      "metadata": {
        "id": "3ADg5r2ysL1B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}